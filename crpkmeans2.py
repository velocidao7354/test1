# -*- coding: utf-8 -*-
"""CRPKMeans2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mcDZimMgHpx_yQQaCnvv6to2mHhBrD8U

# Crime prediction

# Decision tree
"""

from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
import numpy as np
!pip install update pandas
import pandas as pd

df=pd.read_csv('communities-crime-clean.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.1, 1, 0)

pos=df[(df['highCrime'] == 1)]
pos_percentage=len(pos)/len(df)
neg_percentage=1-pos_percentage
print('positive instance percentage is ',pos_percentage)
print('negative instance percentage is ',neg_percentage)

from sklearn.model_selection import cross_val_score
from sklearn import tree
initial=pd.read_csv('communities-crime-clean.csv')
initial = initial.drop('communityname', 1)
initial = initial.drop('ViolentCrimesPerPop', 1)
initial = initial.drop('fold', 1)

initial = initial.drop('state', 1)
Y = df['highCrime']
clf = tree.DecisionTreeClassifier(max_depth=3)
# clf = tree.DecisionTreeClassifier()
clf = clf.fit(initial, Y)
clf
y_pred = clf.predict(initial)
list(initial)
feature_name=list(initial)
import pydotplus 
from IPython.display import Image 
classname=['High','Low']
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=list(initial),  
                         class_names=classname,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png())

from sklearn.model_selection import cross_val_score
fold=df['fold']
scores = cross_val_score(clf, initial, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores) 
print('cross_val_accuracy_avg is ',np.array(scores).mean()) 
scores = cross_val_score(clf, initial, Y,fold,'precision',10)
print('cross_val_precision is ',scores) 
print('cross_val_precision_avg is ',np.array(scores).mean()) 
scores = cross_val_score(clf, initial, Y,fold,'recall',10)
print('cross_val_recall is ',scores) 
print('cross_val_recall_avg is ',np.array(scores).mean())

from sklearn.metrics import accuracy_score
print ('Accuracy is', accuracy_score(Y,y_pred)*100)
from sklearn.metrics import precision_score
print ('Precesion is', precision_score(Y,y_pred)*100)
from sklearn.metrics import recall_score
print ('Recall is', recall_score(Y,y_pred)*100)

y=[]
x=[]
for i in range (1,16):
    clf = tree.DecisionTreeClassifier(max_depth=i)
    clf = clf.fit(initial, Y)
    y_pred = clf.predict(initial)
    scores = cross_val_score(clf, initial, Y,None,'accuracy',cv=10)
    y.append(np.array(scores).mean())
    x.append(i)

    
plt.plot(x, y)

plt.show()

print('',y)

"""According to the plot above, we can get the best number of max_depth feeding in DecisionTreeClassifier. With the increasing number of max_depth, the mean of cross_val_score_accuracy keeps growing up and then starts to decline on number 3.
Therefore, max_depth =3 in DecisionTreeClassifier can get the best performance to analyze the dataset.
We pick at most four biggest information gain in feature_importances array, which is used gini method to calculate and then get four features shown below.
"""

feature_selection = clf.feature_importances_ 
   

ind = np.argpartition(feature_selection, -4)[-4:]

print('ind is ',ind)
print('4_max_normalized_feature is ',feature_selection[ind])

for x in range(0, len(ind)):
    index=ind[x]
    print(index)
    print('feature_name[index] is ',feature_name[index])

"""Because cross-validation requires that we train on a reduced size dataset and then test on data we did not train against the average accuracy, precision and recall scores drop compared with the full dataset train and test.

# NaiveBayes-Gaussian
"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(initial, Y).predict(initial)

print("mislabel num is ",(Y != y_pred).sum())

# print ('sigma is ',gnb.sigma_)
variance=gnb.sigma_
stand_deviation =np.sqrt( variance)
# print('standard deviation is',stand_deviation);
sum_standard=stand_deviation[0]+stand_deviation[1]
# print('sum of standard deviation is',sum_standard);

# print ('theta is ',gnb.theta_)
mean=gnb.theta_
difference=mean[0]-mean[1]
# print('difference is ',abs(difference))
normalized_feature=abs(difference)/sum_standard
# print('normalized_feature is ',normalized_feature)


ind = np.argpartition(normalized_feature, -10)[-10:]

print('ind is ',ind)
print('10_max_normalized_feature is ',normalized_feature[ind])

for x in range(0, len(ind)):
    index=ind[x]
    print(index)
    print('feature_name[index] is ',feature_name[index])





from sklearn.model_selection import cross_val_score
fold=df['fold']
scores = cross_val_score(gnb, initial, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores) 
print('cross_val_accuracy_avg is ',np.array(scores).mean()) 
scores = cross_val_score(gnb, initial, Y,fold,'precision',10)
print('cross_val_precision is ',scores) 
print('cross_val_precision_avg is ',np.array(scores).mean()) 
scores = cross_val_score(gnb, initial, Y,fold,'recall',10)
print('cross_val_recall is ',scores) 
print('cross_val_recall_avg is ',np.array(scores).mean())

"""$\dfrac{|μ_T-μ_F |}{(σ_T+σ_F )}$, the measure of choosing predictive feature, is derived by Correlation coefficient formula, $\dfrac{Cov(X,Y)}{σ_Xσ_Y}$. According to the definition of Correlation coefficient, in order to standardlize the covariance which is really sensetive, we need to divide variance to get the indicator of Correlation coefficient between X and Y. Therefore, $\dfrac{|μ_T-μ_F |}{(σ_T+σ_F )}$,the absolute correlation coefficient , can estimate the strength of the relationship between the attribute and T/F label. If the number is larger, the relationship is stronger. So it can be the criteria to find out predictive feature.

Naive Bayes on average has worse accuracy and recall than Decision Tree on the dataset.  This is likely due to correlations between features in the dataset that by definition the Naive Bayes classifier assumes are conditionally independent. Interestingly, both find PctKids2Par to be the most predictive feature.

# LinearSVC
"""

from sklearn import svm
lin_svc = svm.LinearSVC(C=0.01447, penalty="l1", dual=False).fit(initial, Y)
# using L1-norm (sparsity method) to make unless feature weight become 0 , C value increase->more complex model having more weight
feature_weight=abs(lin_svc.coef_[0])
print("",feature_weight)
for i in range(0,len(feature_weight)):
    if(feature_weight[i]!=0):
        print('select_feature_is ',feature_name[i], ' feature_weight is ', feature_weight[i])



from sklearn.model_selection import cross_val_score
fold=df['fold']
scores = cross_val_score(lin_svc, initial, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores) 
print('cross_val_accuracy_avg is ',np.array(scores).mean()) 
scores = cross_val_score(lin_svc, initial, Y,fold,'precision',10)
print('cross_val_precision is ',scores) 
print('cross_val_precision_avg is ',np.array(scores).mean()) 
scores = cross_val_score(lin_svc, initial, Y,fold,'recall',10)
print('cross_val_recall is ',scores) 
print('cross_val_recall_avg is ',np.array(scores).mean())

"""In this method of estimating predictive features, we use L1-norm to be a penalty in LinearSVC. L1 is known as least absolute errors to compute the penalty value to be used for adjusting the model. It has a sparsity property that reduces the coeficients for features that are not predictive to zero removing them from the model adjustment calculations. 

The Penalty value, C, represents the degree of model complexity. We tuned C to a low value to pick out at most 11 predictive features. This can be achieved because the L1 model is more likely to reduce coefficients of features to zero when a lower C value is set. racePctWhite, racePctHisp, PctKids2Par, PctIlleg, FemalePctDiv and TotalPctDiv are all consistent features with the previous models. PctPersDenseHous, pctWPubAsst, racepctblack and pctUrban have not been predictive in previous models but they also have low coefficients in the LinearSVC model, so they choices of these features by the model is somewhat arbitrary as they will only contribute slight adjustments to the model to correct for errors.

In comparison with DecisionTree, LinearSVC improves accuracy, precision and recall for this dataset. This could be explained by the fact that LinearSVC is able to find an optimal linear sepparating hyperplane for the dataset, while Decision Tree is only able to use axis alligned planes to splint in a heirarchical fashion to split the dataset. Interestingly LinearSVC has a different most predictive feature than the previous models, TotalPctDiv. PctKids2Par is the second most predictive feature for LinearSVC, so it still maintains high importance. This likely means that TotalPctDiv is more effective as a linear sepparator with non-axis aligned planes.

# Linear Regression
"""

from sklearn import linear_model
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_predict

lr = linear_model.LinearRegression(normalize=True)
Y = df['ViolentCrimesPerPop']
predicted = cross_val_predict(lr, initial, Y, cv=10)
print('10_fold_cv_MSE is ',mean_squared_error(Y, predicted))
y_pred = lr.fit(initial, Y).predict(initial)

print('MSE on training set is ',mean_squared_error(Y, y_pred))

# print('coef is ',lr.coef_)

min=np.min(lr.coef_)
print('min is ',min)
index_min = np.argmin(lr.coef_)
print(index_min)
max=np.max(lr.coef_)
print('max is ',max)
index_max = np.argmax(lr.coef_)
print(index_max)
print('coefficient[min] is ',feature_name[index_min])

print('coefficient[max] is ',feature_name[index_max])

"""# Ridge Regression"""

from sklearn.linear_model import Ridge
from sklearn.linear_model import RidgeCV
# Using RidgeCV to reduce the amount of verfitting
ridge_model = RidgeCV(alphas=[10.0,1.0,0.1, 0.01, 0.001])
print(ridge_model)
ridge_reg_score = cross_val_predict(ridge_model, initial, Y, cv=10)
ridge_fit = ridge_model.fit(initial, Y)
#To get the best alpha
print('Best Alpha: ', ridge_model.alpha_)
ridge_predict = ridge_model.predict(initial)
#accuracy of each Fold
print(ridge_reg_score)
#mean accuracy of 10 Folds
print(ridge_reg_score.mean())

#MSE 10 Fold CV
print('10_fold_cv_MSE is: ',mean_squared_error(Y, ridge_reg_score))

#MSE on the Training set
mse_ridge = np.mean((ridge_predict - Y) ** 2)
print ("Mean Square Error on training set: ", mse_ridge)

"""For this problem there is clearly some overfitting on linear regression when compared with ridge regression since the MSE of ridge regression is worse than linear regression but the CV-MSE of ridge regression is better than that of linear regression. The inability for linear regression to drop feature coefficients to zero in the formula clearly has some effect on the method against this dataset.

# Polynomial Regression
"""

from sklearn.preprocessing import *
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression

pol = PolynomialFeatures(degree = 2)
print(pol)

lin_reg = LinearRegression()
pip = Pipeline([("polynomial Feature", pol),("linear_Regression", lin_reg)])
poly_reg_score = cross_val_predict(pip, initial, Y, cv=10)
p_fit = pip.fit(initial, Y)
p_predict = pip.predict(initial)
#accuracy of each fold
print(poly_reg_score)
#mean accuracy of 10 Folds
print(poly_reg_score.mean())

#MSE 10 Fold CV
print('10_fold_cv_MSE is ',mean_squared_error(Y, poly_reg_score))

#MSE on Training Set
mse_poly = np.mean((p_predict - Y) ** 2)
print ("Mean Square Error on training set: ", mse_poly)

"""The linear model significantly outperforms the quadratic model for this dataset. The true model of the dataset is much more likely to be on the order of linear than quadratic.

# Dirty Data w/ PCA & K-nn
"""

import pandas as pd
import numpy as np
from sklearn import tree
from sklearn.model_selection import cross_val_score

df=pd.read_csv('communities-crime-full.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.1, 1, 0)
Y = df['highCrime']

# print('total len is ',len(Y))
initial=pd.read_csv('communities-crime-full.csv')
initial = initial.drop('communityname', 1)
initial = initial.drop('ViolentCrimesPerPop', 1)
initial = initial.drop('fold', 1)
initial = initial.drop('state', 1)
initial = initial.drop('community', 1)
initial = initial.drop('county', 1)
skipinitialspace = True

feature_name=list(initial)
#initial=initial.convert_objects(convert_numeric=True)
initial = initial.apply(pd.to_numeric, errors='coerce')
New_data=initial.fillna(initial.mean())
# print('before...')
# print(initial)
# print('after...')
# print(New_data)  
clf = tree.DecisionTreeClassifier(max_depth=3)
# clf = tree.DecisionTreeClassifier()
clf = clf.fit(New_data, Y)
clf
fold=df['fold']
scores = cross_val_score(clf, New_data, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores) 
print('cross_val_accuracy_avg is ',np.array(scores).mean()) 
scores = cross_val_score(clf, New_data, Y,fold,'precision',10)
print('cross_val_precision is ',scores) 
print('cross_val_precision_avg is ',np.array(scores).mean()) 
scores = cross_val_score(clf, New_data, Y,fold,'recall',10)
print('cross_val_recall is ',scores) 
print('cross_val_recall_avg is ',np.array(scores).mean())

y=[]
x=[]
for i in range (1,16):
    clf = tree.DecisionTreeClassifier(max_depth=i)
    clf = clf.fit(New_data, Y)
    y_pred = clf.predict(New_data)
    scores = cross_val_score(clf, New_data, Y,None,'accuracy',cv=10)
    y.append(np.array(scores).mean())
    x.append(i)

    
plt.plot(x, y)

plt.show()
print('y is ',y)

feature_selection = clf.feature_importances_ 
   

ind = np.argpartition(feature_selection, -4)[-4:]

print('ind is ',ind)
print('4_max_normalized_feature is ',feature_selection[ind])

for x in range(0, len(ind)):
    index=ind[x]
    print(index)
    print('feature_name[index] is ',feature_name[index])

"""The CV result between clean data and full data with the same parameters results in slightly improved accuracy. This might be explained by missing values being replaced by mean of their respective columns. This reduction of the overall variance of the dataset could be improving performance. The most predictive features remain the same so the dirty features being available to split on cannot be the explanation.

4# PCA&K-NN
"""

from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier

df=pd.read_csv('communities-crime-clean.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.1, 1, 0)
initial=pd.read_csv('communities-crime-clean.csv')
Y = df['highCrime']
fold=df['fold']
state=df['state']
community=df['communityname']
initial = initial.drop('fold', 1)
initial = initial.drop('state', 1)
initial = initial.drop('communityname', 1)
initial = initial.drop('ViolentCrimesPerPop', 1)

x=[]
y=[]
for k in range (1,16):
    y.append([])
for i in range (2,15):
    pca = PCA(n_components=i)
    pca.fit(initial)
    pcdf = pca.transform(initial)
    for j in range (1,16):
        knn = KNeighborsClassifier(j)
        knn.fit(pcdf,Y)
        scores = cross_val_score(knn,pcdf,Y,fold,'accuracy',10)
        y[j-1].append(np.mean(scores))
    x.append(i)

plt.plot(x,y[0],'r-',
         x,y[1],'g-',
         x,y[2],'b-',
         x,y[3],'r--',
         x,y[4],'g--',
         x,y[5],'b--',
         x,y[6],'r-.',
         x,y[7],'g-.',
         x,y[8],'b-.',
         x,y[9],'r:',
         x,y[10],'g:',
         x,y[11],'b:',
         x,y[12],'c-',
         x,y[13],'m-',
         x,y[14],'y-')
plt.show()

"""<ul>
<li>X-axis is number of components</li>
<li>Y-axis is accuracy
<ul>
<li>Red-solid-line is k=1</li>
<li>Green-solid-line is k=2</li>
<li>Blue-solid-line is k=3</li>
<li>Red-dashed-line is k=4</li>
<li>Green-dashed-line is k=5</li>
<li>Blue-dashed-line is k=6</li>
<li>Red-dash-dot-line is k=7</li>
<li>Green-dash-dot-line is k=8</li>
<li>Blue-dash-dot-line is k=9</li>
<li>Red-dotted-line is k=10</li>
<li>Green-dotted-line is k=11</li>
<li>Blue-dotted-line is k=12</li>
<li>Cyan-solid-line is k=13</li>
<li>Magenta-solid-line is k=14</li>
<li>Yellow-solid-line is k=15</li>
</ul></li>
</ul>

Blue dotted line has best performance at n-components=5 meaning k=12
"""

pca = PCA(n_components=5)
pca.fit(initial)
pca.components_

pca.explained_variance_ratio_

pcdf = pca.transform(initial)
pcdf

from sklearn.metrics import f1_score
knn = KNeighborsClassifier(n_neighbors=12)
knn.fit(pcdf,Y)
y_pred = knn.predict(pcdf)
print ('fl score is', f1_score(Y,y_pred,average="binary")*100)
print ('Accuracy is', accuracy_score(Y,y_pred)*100)
print ('Precision is', precision_score(Y,y_pred)*100)
print ('Recall is', recall_score(Y,y_pred)*100)

scores = cross_val_score(knn,pcdf,Y,fold,'accuracy',10)
print ('Cross validation accuracy is', np.mean(scores)*100)
scores = cross_val_score(knn,pcdf,Y,fold,'precision',10)
print ('Cross validation precision is', np.mean(scores)*100)
scores = cross_val_score(knn,pcdf,Y,fold,'recall',10)
print ('Cross validation recall is', np.mean(scores)*100)

"""K-nn is a grouping method using distance as the most important factor for classification of data. This combined with the loss of specific features in PCA dimensionality reduction makes finding reliable predictive features very difficult.

Thus the best estimator of predictive features is the greatest magnitude features of the eigenvector that captures the largest percentage of variance in the dataset.
"""

feature_name=list(initial)

ind = np.argpartition(pca.components_[0], -10)[-10:]

print('ind is ',ind)
print('10_max_vector_components is ',pca.components_[0][ind])

for x in range(0, len(ind)):
    index=ind[x]
    print(index)
    print('vector_component[index] is ',feature_name[index])

"""PCA to K-NN results are similar to Decision Tree.  The most significant features are vastly different from the other methods implemented in this project, this is likely because they aren't derived from a classification algorithm but from the features with greatest variance in a dimensionality reduction algorithm."""

df=pd.read_csv('communities-crime-full.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.1, 1, 0)
Y = df['highCrime']
fold=df['fold']
x=[]
y=[]
for k in range (1,16):
    y.append([])
for i in range (2,15):
    pca = PCA(n_components=i)
    pca.fit(New_data)
    pcdf = pca.transform(New_data)
    for j in range (1,16):
        knn = KNeighborsClassifier(j)
        knn.fit(pcdf,Y)
        scores = cross_val_score(knn,pcdf,Y,fold,'accuracy',10)
        y[j-1].append(np.mean(scores))
    x.append(i)

plt.plot(x,y[0],'r-',
         x,y[1],'g-',
         x,y[2],'b-',
         x,y[3],'r--',
         x,y[4],'g--',
         x,y[5],'b--',
         x,y[6],'r-.',
         x,y[7],'g-.',
         x,y[8],'b-.',
         x,y[9],'r:',
         x,y[10],'g:',
         x,y[11],'b:',
         x,y[12],'c-',
         x,y[13],'m-',
         x,y[14],'y-')
plt.show()

"""<ul>
<li>X-axis is number of components</li>
<li>Y-axis is accuracy
<ul>
<li>Red-solid-line is k=1</li>
<li>Green-solid-line is k=2</li>
<li>Blue-solid-line is k=3</li>
<li>Red-dashed-line is k=4</li>
<li>Green-dashed-line is k=5</li>
<li>Blue-dashed-line is k=6</li>
<li>Red-dash-dot-line is k=7</li>
<li>Green-dash-dot-line is k=8</li>
<li>Blue-dash-dot-line is k=9</li>
<li>Red-dotted-line is k=10</li>
<li>Green-dotted-line is k=11</li>
<li>Blue-dotted-line is k=12</li>
<li>Cyan-solid-line is k=13</li>
<li>Magenta-solid-line is k=14</li>
<li>Yellow-solid-line is k=15</li>
</ul></li>
</ul>

Magenta and yellow solid lines have best overall performance at n-components=5 meaning k=14 or k=15, favoring least complexity k=14 will be chosen.
"""

pca = PCA(n_components=5)
pca.fit(New_data)
pca.components_

pca.explained_variance_ratio_

pcdf = pca.transform(New_data)
pcdf

knn = KNeighborsClassifier(n_neighbors=14)
knn.fit(pcdf,Y)
y_pred = knn.predict(pcdf)
print ('fl score is', f1_score(Y,y_pred,average="binary")*100)
print ('Accuracy is', accuracy_score(Y,y_pred)*100)
print ('Precision is', precision_score(Y,y_pred)*100)
print ('Recall is', recall_score(Y,y_pred)*100)

scores = cross_val_score(knn,pcdf,Y,fold,'accuracy',10)
print ('Cross validation accuracy is', np.mean(scores)*100)
scores = cross_val_score(knn,pcdf,Y,fold,'precision',10)
print ('Cross validation precision is', np.mean(scores)*100)
scores = cross_val_score(knn,pcdf,Y,fold,'recall',10)
print ('Cross validation recall is', np.mean(scores)*100)

"""K-nn is a grouping method using distance as the most important factor for classification of data. This combined with the loss of specific features in PCA dimensionality reduction makes finding reliable predictive features very difficult.

Thus the best estimator of predictive features is the greatest magnitude features of the eigenvector that captures the largest percentage of variance in the dataset.
"""

ind = np.argpartition(pca.components_[0], -10)[-10:]

print('ind is ',ind)
print('10_max_vector_components is ',pca.components_[0][ind])

for x in range(0, len(ind)):
    index=ind[x]
    print(index)
    print('vector_component[index] is ',feature_name[index])

"""PCA to K-NN has improved classification with the addition of the features with missing data. This is likely because new features add some variance that is translated into greater distances between the classes in the reduced dimensionality space.

# PolynomialSVC
"""

df=pd.read_csv('communities-crime-clean.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.1, 1, 0)
Y = df['highCrime']
fold=df['fold']

from sklearn.svm import SVC

x=[]
y=[]
for k in range (1,5):
    y.append([])
for i in range (0,4):
    for j in range (1,5):
        poly_svc = SVC(C=2**i, kernel='poly', degree=j).fit(initial, Y)
        scores = cross_val_score(poly_svc,initial,Y,fold,'accuracy',10)
        y[j-1].append(np.mean(scores))
    x.append(2**i)    

plt.plot(x,y[0],'r-',
         x,y[1],'g-',
         x,y[2],'b-',
         x,y[3],'y-')
plt.show()

"""X-axis is error penalty (C) term
Y-axis is accuracy
Red=Degree 1
Green=Degree 2   
Blue=Degree 3   
Yellow=Degree 4

Degree 2 is picked for best performance against linear model. Error penalty value of 8 is picked for best performance.
"""

poly_svc = SVC(C=8, kernel='poly', degree=2).fit(initial, Y)
scores = cross_val_score(poly_svc,initial,Y,fold,'accuracy',10)
print ('Cross validation accuracy is', np.mean(scores)*100)
scores = cross_val_score(poly_svc,initial,Y,fold,'precision',10)
print ('Cross validation precision is', np.mean(scores)*100)
scores = cross_val_score(poly_svc,initial,Y,fold,'recall',10)
print ('Cross validation recall is', np.mean(scores)*100)

"""There is no reasonable method for evaluation of most predictive features with a Polynomial SVC.

Compared with Linear SVC on this data set a polynomial SVC of degree 2 is able to slightly outperform the linear SVC on classification.
"""

df=pd.read_csv('communities-crime-full.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.1, 1, 0)
Y = df['highCrime']
fold=df['fold']

x=[]
y=[]
for k in range (1,5):
    y.append([])
for i in range (0,4):
    for j in range (1,5):
        poly_svc = SVC(C=2**i, kernel='poly', degree=j).fit(New_data, Y)
        scores = cross_val_score(poly_svc,New_data,Y,fold,'accuracy',10)
        y[j-1].append(np.mean(scores))
    x.append(2**i)    

plt.plot(x,y[0],'r-',
         x,y[1],'g-',
         x,y[2],'b-',
         x,y[3],'y-')
plt.show()

"""<ul>
<li>X-axis is error penalty (C) term</li>
<li>Y-axis is accuracy<ul>
<li>Red=Degree 1</li>
<li>Green=Degree 2</li>
<li>Blue=Degree 3</li>
<li>Yellow=Degree 4</li>
</ul></li>
</ul>

Degree 2 is picked for best performance against linear model. Error penalty value of 4 is picked for best performance before significantly diminishing returns.
"""

poly_svc = SVC(C=4, kernel='poly', degree=2).fit(New_data, Y)
scores = cross_val_score(poly_svc,New_data,Y,fold,'accuracy',10)
print ('Cross validation accuracy is', np.mean(scores)*100)
scores = cross_val_score(poly_svc,New_data,Y,fold,'precision',10)
print ('Cross validation precision is', np.mean(scores)*100)
scores = cross_val_score(poly_svc,New_data,Y,fold,'recall',10)
print ('Cross validation recall is', np.mean(scores)*100)

"""There is no reasonable method for evaluation of most predictive features with a Polynomial SVC.

The polynomial SVC is fairly resilient to the missing data in the dirty data set losing very little accuracy and precision and gaining recall. However the plotting output indicates that a linear SVC would likely outperform the polynomial SVC on the dirty data.

# New Threshold
"""

df=pd.read_csv('communities-crime-clean.csv')
# df.describe()
df['ViolentCrimesPerPop'].plot.hist()
print('average is ',np.average(df['ViolentCrimesPerPop']))
Q1, median, Q3 = np.percentile(df['ViolentCrimesPerPop'], [25, 50, 75])
print("Q1(25%) is ",Q1)
print("median is ",median)
print("Q3(75%) is ",Q3)
print("average between Q3 and Q1 is ",(Q3+Q1)/2)
plt.show()
plt.boxplot(df['ViolentCrimesPerPop'])
plt.show()

"""According to histogram and boxplot, we can know how "ViolentCrimesPerPop" distributes in the dataset. And there are lots of outliers following in second part of the dataset, so mean(0.2379) is not suitable to be a threshold.
Then, based on quartile deviation method to retrive Q1, median and Q3, median is different with average between Q1 and Q3. Since the problem of outlier is removed from distance from Q1 to Q3, average between Q1 and Q3 is more suitable than median to be a new threshold. Therefore, 0.2, average between Q1 and Q3, is the most useful threshold.

# New Threshold in DecisionTree
"""

df=pd.read_csv('communities-crime-full.csv')
df
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.2, 1, 0)
Y = df['highCrime']
clf = tree.DecisionTreeClassifier(max_depth=3)
# clf = tree.DecisionTreeClassifier()
clf = clf.fit(New_data, Y)
clf
fold=df['fold']
scores = cross_val_score(clf, New_data, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores) 

print ('Cross validation accuracy is', np.mean(scores)*100)
scores = cross_val_score(clf,New_data,Y,fold,'precision',10)
print ('Cross validation precision is', np.mean(scores)*100)
scores = cross_val_score(clf,New_data,Y,fold,'recall',10)
print ('Cross validation recall is', np.mean(scores)*100)

import matplotlib.pyplot as plt
y=[]
x=[]
for i in range (1,16):
    clf = tree.DecisionTreeClassifier(max_depth=i)
    clf = clf.fit(New_data, Y)
    y_pred = clf.predict(New_data)
    scores = cross_val_score(clf, New_data, Y,None,'accuracy',cv=10,n_jobs = -1)
    y.append(np.array(scores).mean())
    x.append(i)

    
plt.plot(x, y)

plt.show()
print('y is ',y)

classname=['High','Low']
clf = tree.DecisionTreeClassifier(max_depth=3)

clf = clf.fit(New_data, Y)
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=list(New_data),  
                         class_names=classname,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png())

df=pd.read_csv('communities-crime-clean.csv')
df
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.2, 1, 0)
Y = df['highCrime']


clf = tree.DecisionTreeClassifier(max_depth=3)

clf = clf.fit(initial, Y)
clf
import pydotplus 
from IPython.display import Image 
classname=['High','Low']
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=list(initial),  
                         class_names=classname,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png())

fold=df['fold']
scores = cross_val_score(clf, initial, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores) 

print ('Cross validation accuracy is', np.mean(scores)*100)
scores = cross_val_score(clf,initial,Y,fold,'precision',10)
print ('Cross validation precision is', np.mean(scores)*100)
scores = cross_val_score(clf,initial,Y,fold,'recall',10)
print ('Cross validation recall is', np.mean(scores)*100)

# y_pred = clf.predict(initial)
# print ('Accuracy is', accuracy_score(Y,y_pred)*100)
# from sklearn.metrics import precision_score
# print ('Precesion is', precision_score(Y,y_pred)*100)
# from sklearn.metrics import recall_score
# print ('Recall is', recall_score(Y,y_pred)*100)

y=[]
x=[]

for i in range (1,16):
    clf = tree.DecisionTreeClassifier(max_depth=i)
    clf = clf.fit(initial, Y)
    y_pred = clf.predict(initial)
    scores = cross_val_score(clf, initial, Y,None,'accuracy',cv=10)
    y.append(np.array(scores).mean())
    x.append(i)

    
plt.plot(x, y)

plt.show()
print('y is ',y)

"""Using the new threshold a single feature PctIlleg becomes highly predictive of the dataset, but the rest of the possible features to split on lose their effectiveness as linear separators. As such the most effective Decision Tree with the new threshold is depth 1. It would seem that Decision Tree on this dataset greatly benefits from taking a more informed approach to deciding a threshold.

# New Threshold in Naive Bayes Gaussian
"""

gnb = GaussianNB()
y_pred = gnb.fit(initial, Y).predict(initial)

print("mislabel num is ",(Y != y_pred).sum())

# print ('sigma is ',gnb.sigma_)
variance=gnb.sigma_
stand_deviation =np.sqrt( variance)
# print('standard deviation is',stand_deviation);
sum_standard=stand_deviation[0]+stand_deviation[1]
# print('sum of standard deviation is',sum_standard);

# print ('theta is ',gnb.theta_)
mean=gnb.theta_
difference=mean[0]-mean[1]
# print('difference is ',abs(difference))
normalized_feature=abs(difference)/sum_standard
print('normalized_feature is ',normalized_feature)


ind = np.argpartition(normalized_feature, -10)[-10:]

print('ind is ',ind)
print('10_max_normalized_feature is ',normalized_feature[ind])

for x in range(0, len(ind)):
    index=ind[x]
    print(index)
    print('feature_name[index] is ',feature_name[index])





from sklearn.model_selection import cross_val_score
fold=df['fold']
scores = cross_val_score(gnb, initial, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',np.mean(scores)*100) 
scores = cross_val_score(gnb, initial, Y,fold,'precision',10)
print('cross_val_precision is ',np.mean(scores)*100) 
scores = cross_val_score(gnb, initial, Y,fold,'recall',10)
print('cross_val_recall is ',np.mean(scores)*100)

"""For Naive Bayes performance improves relative to the negative class, but drops relative to the positive class with the new threshold. The most predictive feature has stayed the same, PctIlleg. This can be explained by the fact that the change in threshold is relatively small, meaning the distribution of the true and false classes was minimally changed."""

df=pd.read_csv('communities-crime-full.csv')
df
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.2, 1, 0)
Y = df['highCrime']
gnb = GaussianNB()
y_pred = gnb.fit(New_data, Y).predict(New_data)

print("mislabel num is ",(Y != y_pred).sum())

# print ('sigma is ',gnb.sigma_)
variance=gnb.sigma_
stand_deviation =np.sqrt( variance)
# print('standard deviation is',stand_deviation);
sum_standard=stand_deviation[0]+stand_deviation[1]
# print('sum of standard deviation is',sum_standard);

# print ('theta is ',gnb.theta_)
mean=gnb.theta_
difference=mean[0]-mean[1]
# print('difference is ',abs(difference))
normalized_feature=abs(difference)/sum_standard
print('normalized_feature is ',normalized_feature)


ind = np.argpartition(normalized_feature, -10)[-10:]

print('ind is ',ind)
print('10_max_normalized_feature is ',normalized_feature[ind])

for x in range(0, len(ind)):
    index=ind[x]
    print(index)
    print('feature_name[index] is ',feature_name[index])


fold=df['fold']
scores = cross_val_score(gnb, New_data, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',np.mean(scores)*100) 
scores = cross_val_score(gnb, New_data, Y,fold,'precision',10)
print('cross_val_precision is ',np.mean(scores)*100) 
scores = cross_val_score(gnb, New_data, Y,fold,'recall',10)
print('cross_val_recall is ',np.mean(scores)*100)

"""Compared with the clean dataset Naive Bayes performs quite badly, this is likely because the replacement of missing data moves the variance and mean of the two classes much closer together.

# New Threshold in LinearSVC
"""

df=pd.read_csv('communities-crime-clean.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.2, 1, 0)
Y = df['highCrime']
lin_svc = svm.LinearSVC(C=0.014, penalty="l1", dual=False).fit(initial, Y)
# using L1-norm (sparsity method) to make unless feature weight become 0 , C value increase->more complex model having more weight
feature_weight=abs(lin_svc.coef_[0])
for i in range(0,len(feature_weight)):
    if(feature_weight[i]!=0):
        print('select_feature_is ',feature_name[i],' feature_weight_is ',feature_weight[i])



from sklearn.model_selection import cross_val_score
fold=df['fold']
scores = cross_val_score(lin_svc, initial, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores,' average ',np.mean(scores)*100) 
scores = cross_val_score(lin_svc, initial, Y,fold,'precision',10)
print('cross_val_precision is ',scores,' average ',np.mean(scores)*100) 
scores = cross_val_score(lin_svc, initial, Y,fold,'recall',10)
print('cross_val_recall is ',scores,' average ',np.mean(scores)*100)

"""For LinearSVC the performance drops compared with same model against the previous threshold. This model does not seem to benefit from our approach to determining a relevant threshold. Interestingly it has the same most predictive feature as the DecisionTree with the new threshold."""

df=pd.read_csv('communities-crime-full.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.2, 1, 0)
Y = df['highCrime']
fold=df['fold']
feature_name=list(New_data)
lin_svc = svm.LinearSVC(C=0.014, penalty="l1", dual=False).fit(New_data, Y)
# using L1-norm (sparsity method) to make unless feature weight become 0 , C value increase->more complex model having more weight
feature_weight=abs(lin_svc.coef_[0])
for i in range(0,len(feature_weight)):
    if(feature_weight[i]!=0):
        print('select_feature_is ',feature_name[i],' feature_weight_is ',feature_weight[i])



from sklearn.model_selection import cross_val_score
fold=df['fold']
scores = cross_val_score(lin_svc, New_data, Y,fold,'accuracy',10)
print('cross_val_accuracy is ',scores,' average ',np.mean(scores)*100) 
scores = cross_val_score(lin_svc, New_data, Y,fold,'precision',10)
print('cross_val_precision is ',scores,' average ',np.mean(scores)*100) 
scores = cross_val_score(lin_svc, New_data, Y,fold,'recall',10)
print('cross_val_recall is ',scores,' average ',np.mean(scores)*100)

"""Compared with the clean dataset on the new threshold the most predictive features remain constant but the additional dimensions of the features with missing data appears to improve accuracy, precision and recall but is still worse than the original threshold linear SVM on clean data.

# Decision Forest & K-Means

Decision Forest using a Random Forest algorithm. Nodes are chosen from a the best split of a random subset of the features and the best of a set of random thresholds for those features.
"""

from sklearn.ensemble import RandomForestClassifier
df=pd.read_csv('communities-crime-clean.csv')
df['highCrime'] = np.where(df['ViolentCrimesPerPop']>0.1, 1, 0)
initial=pd.read_csv('communities-crime-clean.csv')
initial = initial.drop('communityname', 1)
initial = initial.drop('ViolentCrimesPerPop', 1)
initial = initial.drop('fold', 1)

initial = initial.drop('state', 1)
fold=df['fold']
Y = df['highCrime']
feature_name=list(initial)
clf = RandomForestClassifier(n_estimators=10,max_features='sqrt')
clf = clf.fit(initial, Y)
# dot_data = tree.export_graphviz(clf, out_file=None, 
#                          feature_names=list(initial),  
#                          class_names='highCrime',  
#                          filled=True, rounded=True,  
#                          special_characters=True)  
# graph = pydotplus.graph_from_dot_data(dot_data)  
# Image(graph.create_png())

y_pred = clf.predict(initial)
feature_importance=clf.feature_importances_
print("feature importance is ",feature_importance)
print ("length feature array",len(feature_importance))
for i in range (0,len(feature_importance)):
    if(feature_importance[i]!=0):
        print("index is ",feature_name[i])

index_max = np.argmax(feature_importance)
print(index_max)

print('coefficient[max] is ',feature_name[index_max])

print ('fl score is', f1_score(Y,y_pred,average="binary")*100)
print ('Accuracy is', accuracy_score(Y,y_pred)*100)
print ('Precesion is', precision_score(Y,y_pred)*100)
print ('Recall is', recall_score(Y,y_pred)*100)

scores = cross_val_score(clf,initial,Y,fold,'accuracy',10)
print ('Cross validation accuracy is', np.mean(scores)*100)
scores = cross_val_score(clf,initial,Y,fold,'precision',10)
print ('Cross validation precision is', np.mean(scores)*100)
scores = cross_val_score(clf,initial,Y,fold,'recall',10)
print ('Cross validation recall is', np.mean(scores)*100)

"""Performance is relatively consistent with that of the standard Decision Tree. The most predictive feature is the same as Decision Tree, PctKids2Par. Random Decision Forest does not seem to offer much for the increase in complexity of the model."""

import sklearn.metrics as metrics
from sklearn.cluster import KMeans
communities_crime_df = pd.read_csv('communities-crime-clean.csv')

# Sanity test we have good data
communities_crime_df.head()

def setHighCrime(df):
    '''Function to set value of highCrime depending on ViolentCrimesPerPop'''
    if df['ViolentCrimesPerPop'] > 0.1:
        return True
    else:
        return False
    
# Adding a new field "highCrime"
communities_crime_df['highCrime'] = communities_crime_df.apply(setHighCrime, axis=1)

# Calculating the percentage of positive and negative instances in the dataset
percentage_intances = communities_crime_df.groupby('highCrime').size() * 100 / len(communities_crime_df)
print(percentage_intances)
print("------------------")
print("Percentage Positive Instance = {}\nPercentage Negative Instance = {} ".format(percentage_intances[1],percentage_intances[0]))

#Dropping non-predictive fields as well as ViolentCrimesPerPop field 
X = communities_crime_df.drop('ViolentCrimesPerPop', axis=1).drop('state', axis=1).drop('communityname', axis=1).drop('fold', axis=1).drop('highCrime', axis=1)
features = list(X.columns)
y = communities_crime_df["highCrime"]

# Applying polynomial Kernel SVC
poly_clf = svm.SVC(kernel='poly', degree=2, C= 50)
X_d = X
y_d = y
# For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly.
# non clean dataset 
poly_accuracy_d = cross_val_score(poly_clf, X_d, y_d, cv=10, scoring='accuracy').mean()
poly_precision_d = cross_val_score(poly_clf, X_d, y_d, cv=10, scoring='precision').mean()
poly_recall_d = cross_val_score(poly_clf, X_d, y_d, cv=10, scoring='recall').mean()

print ('Accuracy for polynomial(Dirty Data) is', poly_accuracy_d)
print ('Precision for polynomial(Dirty Data) is', poly_precision_d)
print ('Recall for polynomial(Dirty Data) is', poly_recall_d)

# clean dataset
X = communities_crime_df.drop('ViolentCrimesPerPop', axis=1).drop('state', axis=1).drop('communityname', axis=1).drop('fold', axis=1).drop('highCrime', axis=1)
features = list(X.columns)
y = communities_crime_df["highCrime"]

poly_accuracy = cross_val_score(poly_clf, X, y, cv=10, scoring='accuracy').mean()
poly_precision = cross_val_score(poly_clf, X, y, cv=10, scoring='precision').mean()
poly_recall = cross_val_score(poly_clf, X, y, cv=10, scoring='recall').mean()

print ('Accuracy for polynomial(Clean Data) is', poly_accuracy)
print ('Precision for polynomial(Clean Data) is', poly_precision)
print ('Recall for polynomial(Clean Data) is', poly_recall)

#clf = KMeans(n_clusters = 2)
km_clf = KMeans(n_clusters = 2)
km_accuracy_d = cross_val_score(km_clf, X_d, y_d, cv=10, scoring='accuracy').mean()
km_precision_d = cross_val_score(km_clf, X_d, y_d, cv=10, scoring='precision').mean()
km_recall_d = cross_val_score(km_clf, X_d, y_d, cv=10, scoring='recall').mean()

print ('Accuracy for KMeans(Dirty data) is', km_accuracy_d)
print ('Precision for KMeans(Dirty data) is', km_precision_d)
print ('Recall for KMeans(Dirty data) is', km_recall_d)

km_accuracy = cross_val_score(km_clf, X, y, cv=10, scoring='accuracy').mean()
km_precision = cross_val_score(km_clf, X, y, cv=10, scoring='precision').mean()
km_recall = cross_val_score(km_clf, X, y, cv=10, scoring='recall').mean()

print ('Accuracy is for KMeans(Clean data)', km_accuracy)
print ('Precision is for KMeans(Clean data)', km_precision)
print ('Recall is for KMeans(Clean data)', km_recall)

names = ["Nonlinear SVM" ,"K Means"]
acc_list = [poly_accuracy,km_accuracy]
pre_list = [poly_precision,km_precision]
re_list = [poly_recall,km_recall]
xaxisRange = range(2)
plt.xticks(xaxisRange, names, rotation=45)
# plt.legend()

plt.plot(xaxisRange,acc_list,'ro',color="Red",label="Accuracy")
plt.plot(xaxisRange,pre_list,'>',color="green",label="Precision")
plt.plot(xaxisRange,pre_list,'<',color="green",label="Recall")

plt.xlabel('Model')
plt.ylabel('Metrics')
plt.margins(0.2)
plt.subplots_adjust(bottom=0.15)
legend = plt.legend()
plt.show()

"""Clearly the Polynomial Kernel (non-linear SVM) has better results for metrics as compared to K means, and seems to be most consistently predictive of high crime rates."""